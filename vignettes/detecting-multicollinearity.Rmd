---
title: "Detecting multi-collinearity in regression models"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Detecting multi-collinearity in regression models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(nicethings)
data(solid)
```

*np_multicol should be considered experimental.* 
Really I should shift it to a development branch until I'm happy with it. 

I learned regression modelling at LSHTM, and Neil Pearce taught us modelling strategies there. 
The lectures he gave became a paper with Sander Greenland and Rhian Daniel.[^1] 

A particular focus was the detection of multi-collinearity and nicethings includes a function `np_multicol` that implements Neil's approach. 

Overall, the creation of a causal model is recommended to follow the process below:

1. Variable selection and specification
    + 1.1 Specify a causal model on the basis of a conceptual framework
    + 1.2 Select appropriate variables using a directed acyclic graph
    + 1.3 Divide candidate variables into three groups: main exposure(s), forced variables (those that are expected to be included in any analysis such as age or sex) and non-forced variables 
2. Minimal-model analysis
    + 2.1 Fit a minimal model including only the main exposure and the forced variables
3. Variable selection
    + 3.1 Fit a 'full' model that includes main exposure, forced and non-forced variables
4. Reduction of mean square error
    + 4.1 Enter a loop of reducing the model by dropping non-forced variables and assessing the change in the relative mean square error
    + 4.2 If there are variables that result in a relative mean square error of substantially less than 1, exclude this variable from the model
5. Assessment of effect measure modification
    + 5.1 Assess the effect of possible interactions or effect modifiers

We'll gloss over part one of the process as that could (and probably is) be the subject of an entire book and is not the focus of this article. 
  
Part four of this process is tedious, so I've written a function that adapts an MS Excel sheet my friend Adrian Root wrote to assess relative mean square error between a full and reduced regression model. 
It furthers the analysis by looping through a set of covariates and returning a dataframe output for examining the effect of each non-forced variable. 

First, we load some data, set up a model and a vector of variables to test for multi-collinearity. 

The data are a series of anthropometric measurements based on apparently healthy Indians. 
This data set is shared under a CC-BY statement, with credit to Mohit Aggarwal and Anurag Agrawal. 

To demonstrate the problem, we're going to analyse the relationship between systolic blood pressure and two other variables: waist circumference and weight in kilograms. 

```{r, echo=TRUE}
m1 <- lm(systolic_pressure ~ weight_kg, data = solid)
m2 <- lm(systolic_pressure ~ waist_circumference_cm, data = solid)
m3 <- lm(systolic_pressure ~ weight_kg + waist_circumference_cm, data = solid)

problem <- data.frame(term = c("Weight", "Waist", "Both (Weight)", "Both (Waist)"), 
                      coefficient = c(coef(m1)[2], coef(m2)[2], coef(m3)[2],coef(m3)[3]), 
                      std_error = c(coef(summary(m1))[2,2], coef(summary(m2))[2,2], 
                                    coef(summary(m3))[2,2], coef(summary(m3))[3,2])
                      )
problem
```

So, when we regress on both waist circumference and weight, the coefficient for waist falls by about half and the standard error nearly doubles. 
The increase in the standard error means that we risk assuming that a factor is not significantly associated with the outcome (systolic blood pressure).

```{r, echo=TRUE}
# fit all the variables 
mt_m <- lm(systolic_pressure ~ sex + age_years + height_cm + weight_kg + bmi + waist_circumference_cm + waist_height_ratio, data = solid) 

# create a vector of variables to assess for multi-collinearity
mt_vars <- c("sex", "weight_kg", "waist_circumference_cm", "waist_height_ratio") 
```

Now, we can step through each of the variables and assess the effect it has on the relative root mean square error. 

```{r, echo=TRUE}
np_multicol(model = mt_m, main_effect = "age_years", covariates = mt_vars)
```

Here, we can see that the variable `carb` has a `rel_rmse` that is substantially smaller than 1 and it should be considered whether this variable is suitable for inclusion in the final regression model. 
Other variables, except perhaps `disp`, have values for `rel_rmse` that are close to, or above, 1 and so should not be excluded from the final regression model. 

[^1]: Greenland, S., Daniel, R., & Pearce, N. (2016). Outcome modelling strategies in epidemiology: traditional methods and basic alternatives. International journal of epidemiology, 45(2), 565-575.

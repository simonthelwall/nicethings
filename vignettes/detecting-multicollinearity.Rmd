---
title: "Detecting multi-collinearity in regression models"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Detecting multi-collinearity in regression models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(nicethings)
```

I learned regression modelling at LSHTM, and Neil Pearce taught us modelling strategies there. 
The lectures he gave became a paper with Sander Greenland and Rhian Daniel.[^1] 

A particular focus was the detection of multi-collinearity and nicethings includes a function `np_multicol` that implements Neil's approach. 

Overall, the creation of a causal model is recommended to follow the process below:

1. Variable selection and specification
    + 1.1 Specify a causal model on the basis of a conceptual framework
    + 1.2 Select appropriate variables using a directed acyclic graph
    + 1.3 Divide candidate variables into three groups: main exposure(s), forced variables (those that are expected to be included in any analysis such as age or sex) and non-forced variables 
2. Minimal-model analysis
    + 2.1 Fit a minimal model including only the main exposure and the forced variables
3. Variable selection
    + 3.1 Fit a 'full' model that includes main exposure, forced and non-forced variables
4. Reduction of mean square error
    + 4.1 Enter a loop of reducing the model by dropping non-forced variables and assessing the change in the relative mean square error
    + 4.2 If there are variables that result in a relative mean square error of substantially less than 1, exclude this variable from the model
5. Assessment of effect measure modification
    + 5.1 Assess the effect of possible interactions or effect modifiers

We'll gloss over part one of the process as that could (and probably is) be the subject of an entire book and is not the focus of this article. 
  
Part four of this process is tedious, so I've written a function that adapts an MS Excel sheet my friend Adrian Root wrote to assess relative mean square error between a full and reduced regression model. 
It furthers the analysis by looping through a set of covariates and returning a dataframe output for examining the effect of each non-forced variable. 

First, we load some data, set up a model and a vector of variables to test for multi-collinearity. 

```{r, echo=TRUE}
data(mtcars)

# fit all the variables 
mt_m <- lm(mpg ~ ., data = mtcars) 

# create a vector of variables to assess for multi-collinearity
mt_vars <- names(mtcars) 

# exclude the outcome and the main exposure of interest, in this case, wt
mt_vars <- mt_vars[!mt_vars %in% c("mpg", "wt")] 
```

Now, we can step through each of the variables and assess the effect it has on the relative root mean square error. 

```{r, echo=TRUE}
np_multicol(model = mt_m, main_effect = "wt", covariates = mt_vars)
```

Here, we can see that the variable `carb` has a `rel_rmse` that is substantially smaller than 1 and it should be considered whether this variable is suitable for inclusion in the final regression model. 
Other varaibles, except perhaps `disp`, have values for `rel_rmse` that are close to, or above, 1 and so should not be excluded from the final regression model. 

[^1]: Greenland, S., Daniel, R., & Pearce, N. (2016). Outcome modelling strategies in epidemiology: traditional methods and basic alternatives. International journal of epidemiology, 45(2), 565-575.
